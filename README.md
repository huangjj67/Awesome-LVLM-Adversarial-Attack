# Awesome-LVLM-Adversarial-Attack
Ready to dive into the world of adversarial attacks on Large Vision-Language Models (LVLMs)? üß†üí• This collection of papers and repos is your secret weapon for exploring the wild side of AI! üöÄ From sneaky inputs that confuse multimodal models to mind-bending techniques that expose hidden vulnerabilities, you'll find everything you need to push LVLMs to their limits. ü§ñüí° Whether you're a curious researcher, a coding wizard, or just someone who loves seeing AI get a little "messed with," these resources will inspire you to think outside the box and level up the security and performance of the models we rely on. üîíüîç So, what are you waiting for? Jump in and let's break (and fix) some AI! üòé‚ö°


## Papers
* **On Evaluating Adversarial Robustness of Large Vision-Language Models** | [Github](https://github.com/yunqing-me/AttackVLM) ![Star](https://img.shields.io/github/stars/yunqing-me/AttackVLM.svg?style=social&label=Star)
  * Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin
  * Singapore University of Technology and Design, Sea AI Lab, Tsinghua University, Renmin University of China
  * [NeurIPS2023] https://arxiv.org/abs/2305.16934
* **Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models**
  * Dong Lu, Zhiqiang Wang, Teng Wang, Weili Guan, Hongchang Gao, Feng Zheng
  * Southern University of Science and Technology, The University of Hong Kong, Monash University, Temple University, Peng Cheng Laboratory
  * [ICCV2023] [https://arxiv.org/abs/2307.14061](https://arxiv.org/abs/2307.14061)
* **On the Adversarial Robustness of Multi-Modal Foundation Models** | 
  * Christian Schlarmann, Matthias Hein
  * University of Tubingen
  * [https://arxiv.org/abs/2308.10741](https://arxiv.org/abs/2308.10741)
* **Adversarial Illusions in Multi-Modal Embeddings** | [Github](https://github.com/ebagdasa/adversarial_illusions) ![Star](https://img.shields.io/github/stars/ebagdasa/adversarial_illusions.svg?style=social&label=Star)
  * Tingwei Zhang, Rishi Jha, Eugene Bagdasaryan, Vitaly Shmatikov
  * Cornell University, Cornell Tech
  * [USENIX Security'24] https://arxiv.org/abs/2308.11804
* **Image Hijacks: Adversarial Images can Control Generative Models at Runtime** |[Page](https://image-hijacks.github.io/) | [Github](https://github.com/euanong/image-hijacks) ![Star](https://img.shields.io/github/stars/euanong/image-hijacks.svg?style=social&label=Star)
  * Luke Bailey, Euan Ong, Stuart Russell, Scott Emmons
  * UC Berkeley, Harvard University, University of Cambridge
  * [arXiv2023] https://arxiv.org/abs/2309.00236
* **How Robust is Google's Bard to Adversarial Image Attacks?** | [Github](https://github.com/thu-ml/Attack-Bard) ![Star](https://img.shields.io/github/stars/thu-ml/Attack-Bard.svg?style=social&label=Star)
  * Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu
  * Tsinghua University, RealAI
  * [arXiv2023] https://arxiv.org/abs/2309.11751
* **Misusing Tools in Large Language Models With Visual Adversarial Examples** | 
  * Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes
  * University of California San Diego, University of Washington
  * [arXiv2023] https://arxiv.org/abs/2310.03185
* **VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models** | [Github](https://github.com/ericyinyzy/VLAttack) ![Star](https://img.shields.io/github/stars/ericyinyzy/VLAttack.svg?style=social&label=Star)
  * Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma
  * The Pennsylvania State University, Zhejiang University, Xi‚Äôan Jiaotong University, Dalian University of Technology, Stony Brook University
  * [NeurIPS2023] [https://arxiv.org/abs/2310.04655](https://arxiv.org/abs/2310.04655)
* **How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs** | [Github](https://github.com/UCSC-VLAA/vllm-safety-benchmark) ![Star](https://img.shields.io/github/stars/UCSC-VLAA/vllm-safety-benchmark.svg?style=social&label=Star)
  * Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, Cihang Xie
  * UC Santa Cruz, UNC-Chapel Hill, University of Edinburgh, University of Oxford, AIWaves Inc.
  * [ECCV2024] https://arxiv.org/abs/2311.16101
* **InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models** | [Github](https://github.com/xunguangwang/InstructTA) ![Star](https://img.shields.io/github/stars/xunguangwang/InstructTA.svg?style=social&label=Star)
  * Xunguang Wang, Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang
  * The Hong Kong University of Science and Technology
  * [arXiv2023] https://arxiv.org/abs/2312.01886
* **On the Robustness of Large Multimodal Models Against Image Adversarial Attacks** |
  * Xuanming Cui, Alejandro Aparcedo, Young Kyun Jang, Ser-Nam Lim
  * University of Central Florida
  * [CVPR2024] [https://arxiv.org/abs/2312.03777](https://arxiv.org/abs/2312.03777)
* **Mutual-modality Adversarial Attack with Semantic Perturbation**
  * Jingwen Ye, Ruonan Yu, Songhua Liu, Xinchao Wang
  * National University of Singapore
  * [AAAI2024] [https://arxiv.org/abs/2312.12768](https://arxiv.org/abs/2312.12768)
* **OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization** | 
  * Dongchen Han, Xiaojun Jia, Yang Bai, Jindong Gu, Yang Liu, Xiaochun Cao
  * Sun Yat-sen University, Nanyang Technological University, Tsinghua University, University of Oxford
  * [arXiv2023] https://arxiv.org/abs/2312.04403
* **Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images** | [Github](https://github.com/KuofengGao/Verbose_Images) ![Star](https://img.shields.io/github/stars/KuofengGao/Verbose_Images.svg?style=social&label=Star)
  * Kuofeng Gao, Yang Bai, Jindong Gu, Shu-Tao Xia, Philip Torr, Zhifeng Li, Wei Liu
  * Tsinghua University, Tencent Technology (Beijing),  University of Oxford, Tencent Data Platform, Peng Cheng Laboratory
  * [ICLR2024] https://arxiv.org/abs/2401.11170
* **On the Vulnerability of LLM/VLM-Controlled Robotics** | 
  * Xiyang Wu, Ruiqi Xian, Tianrui Guan, Jing Liang, Souradip Chakraborty, Fuxiao Liu, Brian Sadler, Dinesh Manocha, Amrit Singh Bedi
  * University of Maryland, Army Research Laboratory, University of Central Florida
  * [arXiv2024] https://arxiv.org/abs/2402.10340
* **The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative** | [Github](https://github.com/ChengshuaiZhao0/The-Wolf-Within) ![Star](https://img.shields.io/github/stars/ChengshuaiZhao0/The-Wolf-Within.svg?style=social&label=Star)
  * Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Yu Kong, Tianlong Chen, Huan Liu
  * Arizona State University, Michigan State University, Harvard University
  * [CVPRworkshop2024] https://arxiv.org/abs/2402.14859
* **Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images** | [Github](https://github.com/aiPenguin/StopReasoning) ![Star](https://img.shields.io/github/stars/aiPenguin/StopReasoning.svg?style=social&label=Star)
  * Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu
  * Technical University of Munich, Ludwig Maximilian University of Munich, Huawei Munich Research Center, University of Oxford
  * [COLM2024] https://arxiv.org/abs/2402.14899
* **An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models** | [Github](https://github.com/Haochen-Luo/CroPA) ![Star](https://img.shields.io/github/stars/Haochen-Luo/CroPA.svg?style=social&label=Star)
  * Haochen Luo, Jindong Gu, Fengyuan Liu, Philip Torr
  * University of Oxford
  * [ICLR2024] https://arxiv.org/abs/2403.09766
* **An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models**
  *  Haochen Luo, Jindong Gu, Fengyuan Liu, Philip Torr
  * Torr Vision Group, University of Oxford
  * [ICLR2024] [https://openreview.net/forum?id=nc5GgFAvtk](https://openreview.net/forum?id=nc5GgFAvtk)
* **B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions** | [Github](https://github.com/zhanghao5201/B-AVIBench) ![Star](https://img.shields.io/github/stars/zhanghao5201/B-AVIBench.svg?style=social&label=Star)
  * Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Nanning Zheng, Kaipeng Zhang
  * Xi‚Äôan Jiaotong University, Shanghai Artificial Intelligence Laboratory, Osaka University
  * [TIFS2024] https://arxiv.org/abs/2403.09346
* **Efficiently Adversarial Examples Generation for Visual-Language Models under Targeted Transfer Scenarios using Diffusion Models** | [Github](https://github.com/gq-max/AdvDiffVLM) ![Star](https://img.shields.io/github/stars/gq-max/AdvDiffVLM.svg?style=social&label=Star)
  * Qi Guo, Shanmin Pang, Xiaojun Jia, Qing Guo
  * Xi‚Äôan Jiaotong University, Nanyang Technological University, Center for Frontier AI Research
  * [TIFS2024] https://arxiv.org/abs/2404.10335
* **Exploring the Transferability of Visual Prompting for Multimodal Large Language Models** | [Github](https://github.com/zycheiheihei/Transferable-Visual-Prompting) ![Star](https://img.shields.io/github/stars/zycheiheihei/Transferable-Visual-Prompting.svg?style=social&label=Star)
  * Yichi Zhang, Yinpeng Dong, Siyuan Zhan, Tianzan Min, Hang Su, Jun Zhu
  * Tsinghua University, RealAI, Pazhou Laboratory (Huangpu)
  * [CVPR2024] [https://arxiv.org/abs/2404.11207](https://arxiv.org/abs/2404.11207)
* **Adversarial Robustness for Visual Grounding of Multimodal Large Language Models** | 
  * Kuofeng Gao, Yang Bai, Jiawang Bai, Yong Yang, Shu-Tao Xia
  * Tsinghua University, Tencent Security Platform, Peng Cheng Laboratory
  * [ICLRworkshop2024] https://arxiv.org/abs/2405.09981
* **Transferable Multimodal Attack on Vision-Language Pre-training Models** | 
  * Haodi Wang, Kai Dong, Zhilei Zhu, Haotong Qin, Aishan Liu, Xiaolin Fang, Jiakai Wang, Xianglong Liu
  * Southeast University, Data Space Research Institute of Hefei Comprehensive National Science Centre, Beihang University, Southeast University, Zhongguancun Laboratory
  * [S&P2024] https://www.computer.org/csdl/proceedings-article/sp/2024/313000a102/1Ub239H4xyg
* **Dissecting Adversarial Robustness of Multimodal LM Agents** | [Github ](https://github.com/ChenWu98/agent-attack)![Star](https://img.shields.io/github/stars/ChenWu98/agent-attack.svg?style=social&label=Star)
  * Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, Aditi Raghunathan
  * Carnegie Mellon University
  * [ICLR2025] https://arxiv.org/abs/2406.12814
* **Refusing Safe Prompts for Multi-modal Large Language Models** | [Github](https://github.com/Sadcardation/MLLM-Refusal) ![Star](https://img.shields.io/github/stars/Sadcardation/MLLM-Refusal.svg?style=social&label=Star)
  * Zedian Shao, Hongbin Liu, Yuepeng Hu, Neil Zhenqiang Gong
  * Duke University
  * [arXiv2024] [https://arxiv.org/abs/2407.09050](https://arxiv.org/abs/2407.09050)
* **Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models** | [Page](https://chaducheng.github.io/Manipulate-Facing-Threats/)
  * Hao Cheng, Erjia Xiao, Chengyuan Yu, Zhao Yao, Jiahang Cao, Qiang Zhang, Jiaxu Wang, Mengshu Sun, Kaidi Xu, Jindong Gu, Renjing Xu
  * The Hong Kong University of Science and Technology, University of Oxford, Hohai University, Hunan University, Drexel University, Beijing University of Technology
  * [arXiv2024] https://arxiv.org/abs/2409.13174
* **AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models** | [Github](https://github.com/jiamingzhang94/AnyAttack) ![Star](https://img.shields.io/github/stars/jiamingzhang94/AnyAttack.svg?style=social&label=Star)
  * Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Jitao Sang, Dit-Yan Yeung
  * Hong Kong University of Science and Technology, Beijing Jiaotong University, Fudan University, Singapore Management University
  * [CVPR2025] [https://arxiv.org/abs/2410.05346](https://arxiv.org/abs/2410.05346)
* **Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models** | 
  * Yubo Wang, Chaohu Liu, Yanqiu Qu, Haoyu Cao, Deqiang Jiang, Linli Xu
  * University of Science and Technology of China, Tencent YouTu Lab
  * [ACMMM2024] https://arxiv.org/abs/2410.06699
* **Replace-then-Perturb: Targeted Adversarial Attacks With Visual Reasoning for Vision-Language Models**|
  * Jonggyu Jang, Hyeonsu Lyu, Jungyeon Koh, Hyun Jong Yang
  * Seoul National University, Pohang University, Seoul National University
  * [https://arxiv.org/abs/2411.00898](https://arxiv.org/abs/2411.00898)
* **Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models Across Both Images and Text with a Single Perturbation** | 
  * Xuanming Cui, Alejandro Aparcedo, Young Kyun Jang, Ser-Nam Lim
  * Korea Advanced Institute of Science and Technology
  * [arXiv2024] https://arxiv.org/abs/2412.08108
* **Pandora's Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models** | 
  * Daizong Liu, Mingyu Yang, Xiaoye Qu, Pan Zhou, Xiang Fang, Keke Tang, Yao Wan, Lichao Sun
  * Peking University, Huazhong University of Science and Technology, Nanyang Technological University, Guangzhou University, Lehigh University
  * [NeurIPS2024] [https://proceedings.neurips.cc/paper_files/paper/2024/hash/5d516fc09b53e9a7fade4fbad703e686-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2024/hash/5d516fc09b53e9a7fade4fbad703e686-Abstract-Conference.html)
* **Transferability of Adversarial Attacks in Video-based MLLMs: A Cross-modal Image-to-Video Approach** | 
  * Linhao Huang, Xue Jiang, Zhiqiang Wang, Wentao Mo, Xi Xiao, Bo Han, Yongjie Yin, Feng Zheng
  * Tsinghua University, Southern University of Science and Technology, Hong Kong Baptist University, Hong Kong University of Science and Technology, China Electronics Corporation
  * [arXiv2025] https://arxiv.org/abs/2501.01042
* **Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails** | 
  * Yijun Yang, Lichao Wang, Xiao Yang, Lanqing Hong, Jun Zhu
  * Tsinghua University, Huawei Noah‚Äôs Ark Lab
  * [arXiv2025] https://arxiv.org/abs/2502.05772
* **Universal Adversarial Attack on Multimodal Aligned LLMs** | 
  * Temurbek Rahmatullaev, Polina Druzhinina, Matvey Mikhalchuk, Andrey Kuznetsov, Anton Razzhigaev
  * AIRI, MSU, HSE University, Skoltech
  * [arXiv2025] https://arxiv.org/abs/2502.07987
* **Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack** |  
  * Chenhe Gu, Jindong Gu, Andong Hua, Yao Qin
  * University of California, University of Oxford, University of California, Santa Barbara
  * [arXiv2025] https://arxiv.org/abs/2502.19672
* **Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks** | [Github](https://github.com/JarvisUSTC/DiffPure-RobustVLM) ![Star](https://img.shields.io/github/stars/JarvisUSTC/DiffPure-RobustVLM.svg?style=social&label=Star)
  * Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yichen Fu, Yichun Feng, Kin-man Lam
  * University of Science and Technology of China, The Hong Kong Polytechnic University, University of Washington, Nanjing University, Stanford University, University of the Chinese Academy of Sciences
  * [ICCV2025] https://arxiv.org/abs/2504.01308
* **Manipulating Multimodal Agents via Cross-Modal Prompt Injection** |  
  * Le Wang, Zonghao Ying, Tianyuan Zhang, Siyuan Liang, Shengshan Hu, Mingchuan Zhang, Aishan Liu, Xianglong Liu
  * Beihang University, National University of Singapore, Huazhong University of Science and Technology, Henan University of Science and Technology
  * [arXiv2025] https://arxiv.org/abs/2504.14348
* **QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models**|[Github](https://github.com/btzyd/qava) 
  * Yudong Zhang, Ruobing Xie, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Yu Wang
  * Tsinghua University, University of Science and Technology Beijing, University of Macau
  * [NAACL2025] [https://arxiv.org/abs/2504.11038](https://arxiv.org/abs/2504.11038)
* **VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models** | [Github](https://github.com/hefeimei06/VEAttack-LVLM) ![Star](https://img.shields.io/github/stars/hefeimei06/VEAttack-LVLM.svg?style=social&label=Star)
  * Hefei Mei, Zirui Wang, Shen You, Minjing Dong, Chang Xu
  * City University of Hong Kong, University of Sydney
  * [arXiv2025] https://arxiv.org/abs/2505.17440
* **Attention! You Vision Language Model Could Be Maliciously Manipulated** | [Github](https://github.com/Trustworthy-AI-Group/VMA) ![Star](https://img.shields.io/github/stars/Trustworthy-AI-Group/VMA.svg?style=social&label=Star)
  * Xiaosen Wang, Shaokang Wang, Zhijin Ge, Yuyang Luo, Shudong Zhang
  * Huazhong University of Science and Technology, Shanghai Jiaotong University, Xidian University, Brown University
  * [NeurIPS2025] https://arxiv.org/abs/2505.19911
* **Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment** |  [Github](https://github.com/jiaxiaojunQAQ/FOA-Attack) ![Star](https://img.shields.io/github/stars/jiaxiaojunQAQ/FOA-Attack.svg?style=social&label=Star)
  * Xiaojun Jia, Sensen Gao, Simeng Qin, Tianyu Pang, Chao Du, Yihao Huang, Xinfeng Li, Yiming Li, Bo Li, Yang Liu
  * Nanyang Technological University, MBZUAI, Sea AI Lab, University of Illinois Urbana-Champaign
  * [NeurIPS2025] https://arxiv.org/abs/2505.21494
* **Transferable Adversarial Attacks on Black-Box Vision-Language Models**|
  * Kai Hu, Weichen Yu, Li Zhang, Alexander Robey, Andy Zou, Chengming Xu, Haoqi Hu, Matt Fredrikson
  * Carnegie Mellon University
  * [arXiv2025] [https://arxiv.org/abs/2505.01050](https://arxiv.org/abs/2505.01050)
* **Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack** | 
  * Juan Ren, Mark Dras, Usman Naseem
  * Macquarie University
  * [arXiv2025] https://arxiv.org/abs/2505.21967
* **X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP** |  [Github](https://github.com/HanxunH/XTransferBench) ![Star](https://img.shields.io/github/stars/HanxunH/XTransferBench.svg?style=social&label=Star)
  * Hanxun Huang, Sarah Erfani, Yige Li, Xingjun Ma, James Bailey
  * The University of Melbourne, Singapore Management University, Fudan University
  * [ICML2025] https://arxiv.org/abs/2505.05528
* **Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models** | [Page](https://jiamingzhang94.github.io/cavalry/) | [Github](https://github.com/jiamingzhang94/CAVALRY-V) ![Star](https://img.shields.io/github/stars/jiamingzhang94/CAVALRY-V.svg?style=social&label=Star)
  * Atharv Mittal, Agam Pandey, Amritanshu Tiwari, Sukrit Jindal, Swadesh Swain
  * Mehta Family School of Data Science and Artificial Intelligence
  * [MLRC2025] https://arxiv.org/abs/2506.22982
* **CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs** | 
  * Jiaming Zhang, Rui Hu, Qing Guo, Wei Yang Bryan Lim
  * Nanyang Technological University, Beijing Jiaotong University, A*STAR
  * [arXiv2025] https://arxiv.org/abs/2507.00817

* **Resource Consumption Red-Teaming for Large Vision-Language Models** | 
  * Haoran Gao, Yuanhe Zhang, Zhenhong Zhou, Lei Jiang, Fanyu Meng, Yujia Xiao, Kun Wang, Yang Liu, Junlan Feng
  * China Mobile Research Institute, Beijing University of Posts and Telecommunications, Nanyang Technological University, University of Science and Technology of China
  * [arXiv2025] https://arxiv.org/abs/2507.18053
* **VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models**| [Github](https://github.com/hbrachemi/Vlm_defense-attack)
  * Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, and Olivier D¬¥eforges
  *  CREACH Labs, Direction G¬¥en¬¥erale de l‚ÄôArmement
  * [arXiv2025] [https://arxiv.org/abs/2507.08982](https://arxiv.org/abs/2507.08982)
* **Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time** | [Github](https://github.com/Yifan-Lan/Phi) ![Star](https://img.shields.io/github/stars/Yifan-Lan/Phi.svg?style=social&label=Star)
  * Yifan Lan, Yuanpu Cao, Weitong Zhang, Lu Lin, Jinghui Chen
  * The Pennsylvania State University, The University of North Carolina at Chapel Hill
  * [EMNLP2025] https://arxiv.org/abs/2509.12521
* **Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models** | [Page](https://edpa-attack.github.io/) | [Github](https://github.com/trustmlyoungscientist/EDPA_attack_defense) ![Star](https://img.shields.io/github/stars/trustmlyoungscientist/EDPA_attack_defense.svg?style=social&label=Star)
  * Haochuan Xu, Yun Sing Koh, Shuhuai Huang, Zirun Zhou, Di Wang, Jun Sakuma, Jingfeng Zhang
  * The University of Auckland, King Abdullah University of Science and Technology, Tokyo University of Science, RIKEN Center for Advanced Intelligence Project
  * [arXiv2025] https://arxiv.org/abs/2510.13237
* **Transfer Attack for Bad and Good: Explain and Boost Adversarial Transferability across Multimodal Large Language Models**|
  * Hao Cheng, Erjia Xiao, Jiayan Yang, Jinhao Duan, Yichi Wang, Jiahang Cao, Qiang Zhang, Le Yang, Kaidi Xu, Jindong Gu, Renjing Xu
  * The Hong Kong University of Science and Technology, The Chinese University of Hong Kong,  University of North Carolina,  Beijing University of Technology,  Xi‚Äôan Jiaotong University,  Drexel University 
  * [ACMMM2025] [https://dl.acm.org/doi/abs/10.1145/3746027.3755715](https://dl.acm.org/doi/abs/10.1145/3746027.3755715)
* **Imperceptible Transfer Attack on Large Vision-Language Models** | 
  * Xiaowen Cai, Daizong Liu, Runwei Guan, Pan Zhou
  * Huazhong University of Science and Technology, Peking University, University of Liverpool
  * [ICASSP2025] https://ieeexplore.ieee.org/abstract/document/10890065/
* **Towards Building Model/Prompt-Transferable Attackers against Large Vision-Language Models** |  
  * Xiaowen Cai, Daizong Liu, Xiaoye Qu, Xiang Fang, Jianfeng Dong, Keke Tang, Pan Zhou, Lichao Sun, Wei Hu
  * Huazhong University of Science and Technology, Wuhan University, Nanyang Technological University, Zhejiang Gongshang University, Guangzhou University, Lehigh University, Peking University
  * [NeurIPS2025] https://openreview.net/pdf?id=TyW1V1KukG
* **Fit the Distribution: Cross-Image/Prompt Adversarial Attacks on Multimodal Large Language Models** |  
  * Hai Yan, Haijian Ma, Xiaowen Cai, Daizong Liu, Zenghui Yuan, Xiaoye Qu, Jianfeng Dong, Runwei Guan, Xiang Fang, Hongyang He, Yulai Xie, Pan Zhou
  * Huazhong University of Science and Technology, Wuhan University, Zhejiang Gongshang University, The Hong Kong University of Science and Technology, Nanyang Technological University, University of Warwick
  * [NeurIPS2025] https://openreview.net/pdf?id=kn0AyMYw0v
* **ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents** | [Page](https://advedm.github.io/demo/)
  * Yichen Wang, Hangtao Zhang, Hewen Pan, Ziqi Zhou, Xianlong Wang, Peijin Guo, Lulu Xue, Shengshan Hu, Minghui Li, Leo Yu Zhang
  *  Huazhong University, City University of HongKong, Griffith University
  * [NeurIPS2025] [https://arxiv.org/pdf/2509.16645](https://arxiv.org/pdf/2509.16645)
* **HQA-VLAttack: Towards High Quality Adversarial Attack on Vision-Language Pre-Trained Models**|
  * Han Liu, Jiaqi Li, Zhi Xu, Xiaotong Zhang, Xiaoming Xu, Fenglong Ma, Yuanman Li, Hong Yu
  *  Dalian University of Technology, The Pennsylvania State University,  Shenzhen University
  * [NeurIPS2025] [https://openreview.net/forum?id=LZ4IKybwWl)](https://openreview.net/forum?id=LZ4IKybwWl)
* **Onthe Feasibility of Hijacking MLLMs‚Äô Decision Chain via One Perturbation**|
  * Changyue Li, Jiaying Li, Youliang Yuan, Jiaming He, Zhicong Huang, Pinjia He
  *  The Chinese University of Hong Kong, University of Electronic Science and Technology of China, Ant Group
  * [arXiv2025] [https://arxiv.org/abs/2511.20002](https://arxiv.org/abs/2511.20002)
