# Awesome-LVLM-Adversarial-Attack

Ready to dive into the world of adversarial attacks on Large Vision-Language Models (LVLMs)? üß†üí• This collection of papers and repos is your secret weapon for exploring the wild side of AI! üöÄ From sneaky inputs that confuse multimodal models to mind-bending techniques that expose hidden vulnerabilities, you'll find everything you need to push LVLMs to their limits. ü§ñüí° Whether you're a curious researcher, a coding wizard, or just someone who loves seeing AI get a little "messed with," these resources will inspire you to think outside the box and level up the security and performance of the models we rely on. üîíüîç So, what are you waiting for? Jump in and let's break (and fix) some AI! üòé‚ö°

<div align=center><img src="./img/paper_history.png" width="90%" height="90%" /></div>

| Title                                                        |  Setting  |   Type   |      Method      |         **Key-Focus**         |                **Scenario**                 |
| :----------------------------------------------------------- | :-------: | :------: | :--------------: | :---------------------------: | :-----------------------------------------: |
| V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs | Black-box |  Target  |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| ADVLA: Attention-Guided, Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Model | Gray-box  | Untarget |  Gradient-based  |   Efficiency, Stealthiness    |                 Embodied Ai                 |
| On the Feasibility of Hijacking MLLMs‚Äô Decision Chain via One Perturbation | White-box |  Target  |  Gradient-based  |         Universality          |       Autonomous Driving, Embodied Ai       |
| Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval | Black-box |  Target  | Typography-based |          Efficiency           |            image-text retrieval             |
| Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models | Gray-box  | Untarget |  Gradient-based  | Universality, Transferability |                 Embodied Ai                 |
| Gradient Pruning Interactive Attack for Vision-Language Pre-training Models | Black-box | Untarget |  Gradient-based  |        Transferability        |            image-text retrieval             |
| Universal Camouflage Attack on Vision-Language Models for Autonomous Driving | White-box | Untarget |  Gradient-based  |         Universality          |             Autonomous Driving              |
| Fit the Distribution: Cross-Image/Prompt Adversarial Attacks on Multimodal Large Language Models | Black-box |  Target  |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| HQA-VLAttack: Towards High Quality Adversarial Attack on Vision-Language Pre-Trained Models | Black-box |  Target  |  Gradient-based  |        Transferability        | VQA, Image Captioning, image-text retrieval |
| Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time | White-box |  Target  |  Gradient-based  |         Universality          |            VQA, Image Captioning            |
| ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents | Gray-box  |  Target  |  Gradient-based  |         Stealthiness          |       Autonomous Driving, Embodied Ai       |
| GeoShield: Safeguarding geolocation privacy from vision-language models via adversarial perturbations | Black-box | Untarget |  Gradient-based  |        Transferability        |             Privacy Protection              |
| Towards Mechanistic Defenses Against Typographic Attacks in CLIP | White-box | Untarget | Typography-based |         Stealthiness          |            image-text retrieval             |
| VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models | White-box |  Target  |  Gradient-based  |   Stealthiness, Efficiency    |  Privacy Protection, VQA, Image Captioning  |
| CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs | Black-box | Untarget |  Gradient-based  |  Transferability, Efficiency  |                   others                    |
| Resource Consumption Red-Teaming for Large Vision-Language Models | White-box |  Target  |  Gradient-based  |          Efficiency           |                   others                    |
| VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models | White-box | Untarget |  Gradient-based  |  Transferability, Efficiency  |            VQA, Image Captioning            |
| Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models | White-box |  Target  |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink | Black-box | Untarget |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack | White-box | Untarget |  Gradient-based  |         Stealthiness          |               Vulnerabilities               |
| Transferable Adversarial Attacks on Black-Box Vision-Language Models | Black-box |  Target  |  Gradient-based  | Transferability, Universality |            VQA, Image Captioning            |
| Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment | Black-box |  Target  |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks | White-box | Untarget |  Gradient-based  |         Stealthiness          |               Vulnerabilities               |
| Transfer Attack for Bad and Good: Explain and Boost Adversarial Transferability across Multimodal Large Language Models | Black-box |  Target  | Typography-based |        Transferability        |             Privacy Protection              |
| X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP | Black-box | Untarget |  Gradient-based  | Universality, Transferability | VQA, Image Captioning, image-text retrieval |
| Manipulating Multimodal Agents via Cross-Modal Prompt Injection | Black-box |  Target  |  Gradient-based  |        Transferability        |       Embodied Ai, Autonomous Driving       |
| AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization | White-box | Untarget |  Gradient-based  |          Efficiency           |            VQA, Image Captioning            |
| A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1 | Black-box |  Target  |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| Imperceptible Transfer Attack on Large Vision-Language Models | White-box | Untarget |  Gradient-based  | Transferability, Stealthiness |            VQA, Image Captioning            |
| QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models | Gray-box  | Untarget |  Gradient-based  |         Universality          |            VQA, Image Captioning            |
| Semantic-Aligned Adversarial Evolution Triangle for High-Transferability Vision-Language Attack | White-box | Untarget |  Gradient-based  |        Transferability        | image-text retrieval, Image Captioning, VQA |
| Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models Across Both Images and Text with a Single Perturbation | Gray-box  | Untarget |  Gradient-based  |         Universality          | VQA, Image Captioning, image-text retrieval |
| Pandora‚Äôs Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models | Black-box |  Target  |   Query-based    |         Universality          | VQA, Image Captioning, image-text retrieval |
| Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models | Gray-box  | Untarget |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| AnyAttack: Towards Large-scale Self-supervised Adversarial Attacks on Vision-language Models | White-box |  Target  |  Gradient-based  |        Transferability        |   image-text retrieval, Image Captioning    |
| Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector | Black-box |  Target  |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models | White-box | Untarget |  Gradient-based  |        Transferability        | image-text retrieval, Image Captioning, VQA |
| Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack | Gray-box  |  Target  |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| Typographic Attacks in a Multi-Image Setting                 | Black-box | Untarget | Typography-based |         Stealthiness          |            image-text retrieval             |
| Universal Adversarial Attack on Multimodal Aligned LLMs      | White-box |  Target  |  Gradient-based  |         Universality          |                   others                    |
| Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images | White-box |  Target  |  Gradient-based  |         Universality          |             Privacy Protection              |
| Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails | Black-box |  Target  |   Query-based    |        Transferability        |                   others                    |
| An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models | Black-box |  Target  |  Gradient-based  | Transferability, Universality |            VQA, Image Captioning            |
| One Prompt Word Is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models | White-box | Untarget |  Gradient-based  |          Efficiency           |                   others                    |
| AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning | White-box | Untarget |  Gradient-based  |          Efficiency           |              Image Captioning               |
| Refusing Safe Prompts for Multi-modal Large Language Models  | White-box |  Target  |  Gradient-based  |         Stealthiness          |            VQA, Image Captioning            |
| Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models | Black-box | Untarget | Typography-based |         Universality          |                 Embodied Ai                 |
| One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models | Black-box | Untarget |  Gradient-based  | Universality, Transferability |   image-text retrieval, Image Captioning    |
| Improving Adversarial Transferability of Vision-Language Pre-training Models through Collaborative Multimodal Interaction | Black-box | Untarget |  Gradient-based  |        Transferability        |   image-text retrieval, Image Captioning    |
| Exploring the Transferability of Visual Prompting for Multimodal Large Language Models | Black-box | Untarget |  Gradient-based  |        Transferability        |                   others                    |
| Boosting Transferability in Vision-Language Attacks via Diversification along the Intersection Region of Adversarial Trajectory | Black-box | Untarget |  Gradient-based  |        Transferability        | image-text retrieval, VQA, Image Captioning |
| Medical MLLM Is Vulnerable: Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models | Gray-box  |  Target  |  Gradient-based  |         Universality          |             Privacy Protection              |
| Dissecting Adversarial Robustness of Multimodal LM Agents    | Black-box |  Target  |  Gradient-based  |         Universality          |                 Embodied Ai                 |
| Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with Typography | Black-box |  Target  | Typography-based |        Transferability        |             Autonomous Driving              |
| Efficient Generation of Targeted and Transferable Adversarial Examples for Vision-Language Models via Diffusion Models | Black-box |  Target  |  Gradient-based  |  Efficiency, Transferability  |            VQA, Image Captioning            |
| Adversarial Robustness for Visual Grounding of Multimodal Large Language Models | White-box |  Target  |  Gradient-based  |         Universality          |                   others                    |
| On the Robustness of Large Multimodal Models Against Image Adversarial Attacks | White-box | Untarget |  Gradient-based  |         Universality          |            VQA, Image Captioning            |
| SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation | Black-box | Untarget |  Gradient-based  |        Transferability        |            image-text retrieval             |
| OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization | Black-box | Untarget |  Gradient-based  |        Transferability        |            image-text retrieval             |
| FigStep: Jailbreaking Large Vision-Language Models via Typographic Visual Prompts | Black-box |  Target  | Typography-based |         Stealthiness          |                   others                    |
| How Many Are in This Image? A Safety Evaluation Benchmark for Vision LLMs | Black-box | Untarget |  Gradient-based  |         Universality          |            VQA, Image Captioning            |
| On the Adversarial Robustness of Multi-Modal Foundation Models | White-box |  Target  |  Gradient-based  |         Stealthiness          |            Image Captioning, VQA            |
| Image Hijacks: Adversarial Images can Control Generative Models at Runtime | White-box |  Target  |  Gradient-based  |         Universality          |                   others                    |
| Misusing Tools in Large Language Models with Visual Adversarial Examples | White-box |  Target  |  Gradient-based  |         Stealthiness          |              Privacy Violation              |
| AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning | Gray-box  | Untarget |  Gradient-based  |         Universality          |            image-text retrieval             |
| Adversarial Illusions in Multi-Modal Embeddings              | Black-box |  Target  |  Gradient-based  | Universality, Transferability |        image-text retrieval, others         |
| Exploring Transferability of Multimodal Adversarial Samples for Vision-Language Pre-training Models with Contrastive Learning | Black-box | Untarget |  Gradient-based  |        Transferability        | image-text retrieval, VQA, Image Captioning |
| Downstream-agnostic Adversarial Examples                     | White-box | Untarget |  Gradient-based  | Universality, Transferability |        image-text retrieval, others         |
| Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models | Gray-box  | Untarget |  Gradient-based  |        Transferability        |   image-text retrieval, Image Captioning    |
| Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs | Black-box |  Target  |  Gradient-based  |         Stealthiness          |            VQA, Image Captioning            |
| Visual Adversarial Examples Jailbreak Aligned Large Language Models | White-box |  Target  |  Gradient-based  |         Universality          |            VQA, Image Captioning            |
| Jailbreak in Pieces: Compositional Adversarial Attacks on Multi-Modal Language Models | Gray-box  |  Target  |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| Towards Adversarial Attack on Vision-Language Pre-training Models | White-box | Untarget |  Gradient-based  |        Transferability        |          image-text retrieval, VQA          |
| Defense-Prefix for Preventing Typographic Attacks on CLIP    | White-box | Untarget | Typography-based |         Stealthiness          |        image-text retrieval, others         |
| Rethinking Model Ensemble in Transfer-based Adversarial Attacks | Black-box | Untarget |  Gradient-based  |        Transferability        |                   others                    |
| On Evaluating Adversarial Robustness of Large Vision-Language Models | Black-box |  Target  |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems | Gray-box  |  Target  |  Gradient-based  |        Transferability        |             Autonomous Driving              |
| Towards Building Model/Prompt-Transferable Attackers against Large Vision-Language Models | White-box |  Target  |  Gradient-based  |        Transferability        |            VQA, Image Captioning            |
| Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving | Black-box | Untarget |   Query-based    |         Universality          |             Autonomous Driving              |
| Transferability of Adversarial Attacks in Video-based MLLMs: A Cross-modal Image-to-Video Approach | Black-box | Untarget |  Gradient-based  |        Transferability        |                     VQA                     |
| Attention! Your Vision Language Model Could Be Maliciously Manipulated | White-box |  Target  |  Gradient-based  |         Universality          |             Privacy Protection              |
| Chain of Attack: On the Robustness of Vision-Language Models Against Transfer-Based Adversarial Attacks | Black-box |  Target  |  Gradient-based  |  Transferability, Efficiency  |            VQA, Image Captioning            |
| Replace-then-Perturb: Targeted Adversarial Attacks With Visual Reasoning for Vision-Language Models | White-box |  Target  |  Gradient-based  |         Stealthiness          |            VQA, Image Captioning            |
| SCENETAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments | Black-box |  Target  | Typography-based |  Stealthiness, Universality   |            VQA, Image Captioning            |
| PG-Attack: A Precision-Guided Adversarial Attack Framework Against Vision Foundation Models for Autonomous Driving | Black-box |  Target  |  Gradient-based  |         Stealthiness          |             Autonomous Driving              |
| B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions | Black-box | Untarget |   Query-based    |        Transferability        |            Image-text retrieval             |
| Visual Adversarial Attack on Vision-Language Models for Autonomous Driving | White-box |  Target  |  Gradient-based  |        Transferability        |             Autonomous Driving              |
| Stop Reasoning! When Multimodal LLM with Chain-of-Thought Reasoning Meets Adversarial Image | White-box |  Target  |  Gradient-based  |          Efficiency           |                     VQA                     |
| The Wolf Within: Covert Injection of Malice into MLLM Societies via An MLLM Operative | White-box |  Target  |  Gradient-based  |        Transferability        |                   others                    |
| Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Models | Black-box | Untarget | Typography-based |         Universality          |                     VQA                     |
| On the Vulnerability of LLM/VLM-Controlled Robotics          | Black-box | Untarget |   Query-based    |         Universality          |                 Embodied Ai                 |
| Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks | Black-box |  Target  | Typography-based |         Universality          |            VQA, Image Captioning            |
| How Robust is Google‚Äôs Bard to Adversarial Image Attacks?    | Black-box |  Target  |  Gradient-based  |        Transferability        |              Image Captioning               |
| VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models | Black-box | Untarget |  Gradient-based  |        Transferability        |                     VQA                     |
| Mutual-modality Adversarial Attack with Semantic Perturbation | Black-box | Untarget |  Gradient-based  |        Transferability        |            image-text retrieval             |
| InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models | Gray-box  |  Target  |  Gradient-based  |        Transferability        |                     VQA                     |
| Transferable Multimodal Attack on Vision-Language Pre-training Models | Black-box | Untarget |  Gradient-based  |        Transferability        |            image-text retrieval             |


## Papers

* ![](https://img.shields.io/badge/abs-2025.11-red)**V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs**|[arXiv 2025](https://arxiv.org/abs/2511.20223)|[Github](https://github.com/Summu77/V-Attack) ![Star](https://img.shields.io/github/stars/Summu77/V-Attack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.11-red)**Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models**|[arXiv 2025](https://arxiv.org/abs/2511.21663)
* ![](https://img.shields.io/badge/abs-2025.11-red)**On the Feasibility of Hijacking MLLMs‚Äô Decision Chain via One Perturbation**|[arXiv 2025](https://arxiv.org/abs/2511.20002)
* ![](https://img.shields.io/badge/abs-2025.11-red)**Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval**|[arXiv 2025](https://arxiv.org/abs/2511.05325)
* ![](https://img.shields.io/badge/abs-2025.10-red)**Transfer Attack for Bad and Good: Explain and Boost Adversarial Transferability across Multimodal Large Language Models**|[ACM MM 2025](https://dl.acm.org/doi/abs/10.1145/3746027.3755715)
* ![](https://img.shields.io/badge/abs-2025.10-red)**Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models**|[arXiv 2025](https://arxiv.org/abs/2510.13237)|[Page](https://edpa-attack.github.io/)| [Github](https://github.com/trustmlyoungscientist/EDPA_attack_defense) ![Star](https://img.shields.io/github/stars/trustmlyoungscientist/EDPA_attack_defense.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.10-red)**Gradient Pruning Interactive Attack for Vision-Language Pre-training Models**|[TDSC 2025](https://ieeexplore.ieee.org/abstract/document/11217326)
* ![](https://img.shields.io/badge/abs-2025.09-red)**Universal Camouflage Attack on Vision-Language Models for Autonomous Driving**|[arXiv 2025](https://arxiv.org/abs/2509.20196)
* ![](https://img.shields.io/badge/abs-2025.09-red)**HQA-VLAttack: Towards High Quality Adversarial Attack on Vision-Language Pre-Trained Models**|[NeurIPS 2025](https://openreview.net/forum?id=LZ4IKybwWl)
* ![](https://img.shields.io/badge/abs-2025.09-red)**Fit the Distribution: Cross-Image/Prompt Adversarial Attacks on Multimodal Large Language Models**|[NeurIPS 2025](https://openreview.net/pdf?id=kn0AyMYw0v)
* ![](https://img.shields.io/badge/abs-2025.09-red)**Towards Building Model/Prompt-Transferable Attackers against Large Vision-Language Models** |[NeurIPS 2025](https://openreview.net/pdf?id=TyW1V1KukG)
* ![](https://img.shields.io/badge/abs-2025.09-red)**Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time** |[EMNLP 2025](https://arxiv.org/abs/2509.12521)| [Github](https://github.com/Yifan-Lan/Phi) ![Star](https://img.shields.io/github/stars/Yifan-Lan/Phi.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.09-red)**ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents** |[NeurIPS 2025](https://arxiv.org/pdf/2509.16645)| [Page](https://advedm.github.io/demo/)
* ![](https://img.shields.io/badge/abs-2025.08-red)**Geoshield: Safeguarding geolocation privacy from vision-language models via adversarial perturbations**|[AAAI 2026](https://arxiv.org/abs/2508.03209)|[Github](https://github.com/thinwayliu/Geoshield)![Star](https://img.shields.io/github/stars/thinwayliu/Geoshield.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.08-red)**PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems** |[Arxiv 2025](https://arxiv.org/abs/2508.05167)
* ![](https://img.shields.io/badge/abs-2025.08-red)**Towards Mechanistic Defenses Against Typographic Attacks in CLIP**|[arXiv 2025](https://arxiv.org/abs/2508.20570)
* ![](https://img.shields.io/badge/abs-2025.08-red)**Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector**|[arXiv 2025](https://arxiv.org/abs/2508.13739)
* ![](https://img.shields.io/badge/abs-2025.07-red)**VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models**|[arXiv 2025](https://arxiv.org/abs/2507.08982)| [Github](https://github.com/hbrachemi/Vlm_defense-attack)![Star](https://img.shields.io/github/stars/hbrachemi/Vlm_defense-attack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.07-red)**CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs** | [arXiv 2025](https://arxiv.org/abs/2507.00817)
* ![](https://img.shields.io/badge/abs-2025.07-red)**Resource Consumption Red-Teaming for Large Vision-Language Models** | [arXiv 2025](https://arxiv.org/abs/2507.18053)
* ![](https://img.shields.io/badge/abs-2025.06-red)**Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models** |[MLRC 2025](https://arxiv.org/abs/2506.22982)| [Page](https://jiamingzhang94.github.io/cavalry/) | [Github](https://github.com/jiamingzhang94/CAVALRY-V) ![Star](https://img.shields.io/github/stars/jiamingzhang94/CAVALRY-V.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.05-red)**VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models** |[arXiv 2025](https://arxiv.org/abs/2505.17440)| [Github](https://github.com/hefeimei06/VEAttack-LVLM) ![Star](https://img.shields.io/github/stars/hefeimei06/VEAttack-LVLM.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.05-red)**Attention! You Vision Language Model Could Be Maliciously Manipulated** |[NeurIPS 2025](https://arxiv.org/abs/2505.19911)| [Github](https://github.com/Trustworthy-AI-Group/VMA) ![Star](https://img.shields.io/github/stars/Trustworthy-AI-Group/VMA.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.05-red)**Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment** |[NeurIPS 2025](https://arxiv.org/abs/2505.21494)|[Github](https://github.com/jiaxiaojunQAQ/FOA-Attack) ![Star](https://img.shields.io/github/stars/jiaxiaojunQAQ/FOA-Attack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.05-red)**Transferable Adversarial Attacks on Black-Box Vision-Language Models**|[arXiv 2025](https://arxiv.org/abs/2505.01050)
* ![](https://img.shields.io/badge/abs-2025.05-red)**Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack** |[arXiv 2025](https://arxiv.org/abs/2505.21967)
* ![](https://img.shields.io/badge/abs-2025.05-red)**X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP** |[ICML 2025](https://arxiv.org/abs/2505.05528)|  [Github](https://github.com/HanxunH/XTransferBench) ![Star](https://img.shields.io/github/stars/HanxunH/XTransferBench.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.04-red)**Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks**|[ICCV 2025](https://arxiv.org/abs/2504.01308)| [Github](https://github.com/JarvisUSTC/DiffPure-RobustVLM) ![Star](https://img.shields.io/github/stars/JarvisUSTC/DiffPure-RobustVLM.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.04-red)**AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization**|[arXiv 2025](https://arxiv.org/abs/2504.01735)
* ![](https://img.shields.io/badge/abs-2025.04-red)**Manipulating Multimodal Agents via Cross-Modal Prompt Injection** |[arXiv 2025](https://arxiv.org/abs/2504.14348)
* ![](https://img.shields.io/badge/abs-2025.04-red)**QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models**|[NAACL 2025](https://arxiv.org/abs/2504.11038)|[Github](https://github.com/btzyd/qava) ![Star](https://img.shields.io/github/stars/btzyd/qava.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.03-red)**Imperceptible Transfer Attack on Large Vision-Language Models** |[ICASSP 2025](https://ieeexplore.ieee.org/abstract/document/10890065/)
* ![](https://img.shields.io/badge/abs-2025.03-red)**A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1**|[[NeurIPS 2025](https://arxiv.org/abs/2503.10635)|[Page](https://vila-lab.github.io/M-Attack-Website/)|[Github](https://github.com/VILA-Lab/M-Attack)![Star](https://img.shields.io/github/stars/VILA-Lab/M-Attack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.02-red)**Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails** |[arXiv 2025](https://arxiv.org/abs/2502.05772)
* ![](https://img.shields.io/badge/abs-2025.02-red)**MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models**|[arXiv 2025](https://arxiv.org/abs/2502.08079)
* ![](https://img.shields.io/badge/abs-2025.02-red)**Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images**|[ICLR 2025](https://arxiv.org/abs/2502.16593)
* ![](https://img.shields.io/badge/abs-2025.02-red)**Universal Adversarial Attack on Multimodal Aligned LLMs** |[arXiv 2025](https://arxiv.org/abs/2502.07987)
* ![](https://img.shields.io/badge/abs-2025.02-red)**Typographic Attacks in a Multi-Image Setting**|[NAACL 2025](https://arxiv.org/abs/2502.08193)|[Github](https://github.com/XiaomengWang-AI/Typographic-Attacks-in-a-Multi-Image-Setting) ![Star](https://img.shields.io/github/stars/XiaomengWang-AI/Typographic-Attacks-in-a-Multi-Image-Setting.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2025.02-red)**Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack** |[arXiv 2025](https://arxiv.org/abs/2502.19672)
* ![](https://img.shields.io/badge/abs-2025.01-red)**Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink**|[USENIX Security 2025](https://arxiv.org/abs/2501.15269)|[Hugging Face](https://huggingface.co/RachelHGF/Mirage-in-the-Eyes)
* ![](https://img.shields.io/badge/abs-2025.01-red)**Transferability of Adversarial Attacks in Video-based MLLMs: A Cross-modal Image-to-Video Approach** |[arXiv 2025](https://arxiv.org/abs/2501.01042)
* ![](https://img.shields.io/badge/abs-2025.01-red)**Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving**|[arXiv 2025](https://arxiv.org/abs/2501.13563)
* ![](https://img.shields.io/badge/abs-2024.12-red)**Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models Across Both Images and Text with a Single Perturbation** |[arXiv 2024](https://arxiv.org/abs/2412.08108)
* ![](https://img.shields.io/badge/abs-2024.12-red)**SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments**|[CVPR 2025](https://arxiv.org/abs/2412.00114)|[Github](https://github.com/tsingqguo/scenetap)![Star](https://img.shields.io/github/stars/tsingqguo/scenetap.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.11-red)**Replace-then-Perturb: Targeted Adversarial Attacks With Visual Reasoning for Vision-Language Models**| [arXiv 2024](https://arxiv.org/abs/2411.00898)
* ![](https://img.shields.io/badge/abs-2024.11-red)**Chain of Attack: On the Robustness of Vision-Language Models Against Transfer-Based Adversarial Attacks**|[arXiv 2024](https://arxiv.org/abs/2411.15720)
* ![](https://img.shields.io/badge/abs-2024.11-red)**Visual Adversarial Attack on Vision-Language Models for Autonomous Driving**|[arXiv 2024](https://arxiv.org/abs/2411.18275)
* ![](https://img.shields.io/badge/abs-2024.11-red)**Semantic-Aligned Adversarial Evolution Triangle for High-Transferability Vision-Language Attack**|[TPAMI 2025](https://arxiv.org/abs/2411.02669)|[Github](https://github.com/jiaxiaojunQAQ/SA-AET)![Star](https://img.shields.io/github/stars/jiaxiaojunQAQ/SA-AET.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.10-red)**AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models**|[CVPR 2025](https://arxiv.org/abs/2410.05346)| [Github](https://github.com/jiamingzhang94/AnyAttack) ![Star](https://img.shields.io/github/stars/jiamingzhang94/AnyAttack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.10-red)**Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models**|[ACMMM 2024](https://arxiv.org/abs/2410.06699)
* ![](https://img.shields.io/badge/abs-2024.09-red)**Pandora's Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models** |[NeurIPS2024](https://proceedings.neurips.cc/paper_files/paper/2024/hash/5d516fc09b53e9a7fade4fbad703e686-Abstract-Conference.html)

* ![](https://img.shields.io/badge/abs-2024.09-red)**Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models** |[arXiv 2024](https://arxiv.org/abs/2409.13174)| [Page](https://chaducheng.github.io/Manipulate-Facing-Threats/)
* ![](https://img.shields.io/badge/abs-2024.07-red)**Refusing Safe Prompts for Multi-modal Large Language Models** |[arXiv 2024](https://arxiv.org/abs/2407.09050)| [Github](https://github.com/Sadcardation/MLLM-Refusal) ![Star](https://img.shields.io/github/stars/Sadcardation/MLLM-Refusal.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.07-red)**AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning**|[KDD Workshop 2024](https://arxiv.org/abs/2407.21174)
* ![](https://img.shields.io/badge/abs-2024.07-red)**PG-Attack: A Precision-Guided Adversarial Attack Framework Against Vision Foundation Models for Autonomous Driving** |[CIVPR Workshop 2024](https://arxiv.org/abs/2407.13111)|[Github](https://github.com/fuhaha824/PG-Attack)![Star](https://img.shields.io/github/stars/fuhaha824/PG-Attack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.06-red)**One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models**|[ICCV 2025](https://openaccess.thecvf.com/content/ICCV2025/html/Fang_One_Perturbation_is_Enough_On_Generating_Universal_Adversarial_Perturbations_against_ICCV_2025_paper.html)|[Github](https://github.com/ffhibnese/CPGC_VLP_Universal_Attacks)![Star](https://img.shields.io/github/stars/ffhibnese/CPGC_VLP_Universal_Attacks.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.06-red)**Dissecting Adversarial Robustness of Multimodal LM Agents** |[ICLR 2025](https://arxiv.org/abs/2406.12814)| [Github ](https://github.com/ChenWu98/agent-attack)![Star](https://img.shields.io/github/stars/ChenWu98/agent-attack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.05-red)**Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with Typography**|[arXiv 2024](https://arxiv.org/abs/2405.14169)
* ![](https://img.shields.io/badge/abs-2024.05-red)**Medical MLLM is Vulnerable: Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models** |[Arxiv 2024](https://arxiv.org/abs/2405.20775)| [Github](https://github.com/dirtycomputer/O2M_attack)![Star](https://img.shields.io/github/stars/dirtycomputer/O2M_attack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.05-red)**Adversarial Robustness for Visual Grounding of Multimodal Large Language Models** |[ICLR workshop 2024](https://arxiv.org/abs/2405.09981)
* ![](https://img.shields.io/badge/abs-2024.04-red)**Efficiently Adversarial Examples Generation for Visual-Language Models under Targeted Transfer Scenarios using Diffusion Models** |[TIFS](https://arxiv.org/abs/2404.10335)| [Github](https://github.com/gq-max/AdvDiffVLM) ![Star](https://img.shields.io/github/stars/gq-max/AdvDiffVLM.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.04-red)**Exploring the Transferability of Visual Prompting for Multimodal Large Language Models** |[CVPR 2024](https://arxiv.org/abs/2404.11207)| [Github](https://github.com/zycheiheihei/Transferable-Visual-Prompting) ![Star](https://img.shields.io/github/stars/zycheiheihei/Transferable-Visual-Prompting.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.03-red)**One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models**|[CVPR 2024](https://arxiv.org/abs/2403.01849)|[Github](https://github.com/TreeLLi/APT)![Star](https://img.shields.io/github/stars/TreeLLi/APT.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.03-red)**Improving Adversarial Transferability of Vision-Language Pre-training Models through Collaborative Multimodal Interaction**|[CVPR Workshop 2024](https://arxiv.org/abs/2403.10883)
* ![](https://img.shields.io/badge/abs-2024.03-red)**An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models**| [ICLR 2024](https://arxiv.org/abs/2403.09766)| [Github](https://github.com/Haochen-Luo/CroPA) ![Star](https://img.shields.io/github/stars/Haochen-Luo/CroPA.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.03-red)**Boosting Transferability in Vision-Language Attacks via Diversification Along the Intersection Region of Adversarial Trajectory**|[ECCV 2024](https://arxiv.org/abs/2403.12445)|[Github](https://github.com/SensenGao/VLPTransferAttack)![Star](https://img.shields.io/github/stars/SensenGao/VLPTransferAttack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.03-red)**B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions** | [TIFS](https://arxiv.org/abs/2403.09346)| [Github](https://github.com/zhanghao5201/B-AVIBench) ![Star](https://img.shields.io/github/stars/zhanghao5201/B-AVIBench.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.02-red)**Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model**|[[ECCV 2024](https://arxiv.org/abs/2402.19150)|[Github](https://github.com/ChaduCheng/TypoDeceptions) ![Star](https://img.shields.io/github/stars/ChaduCheng/TypoDeceptions.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.02-red)**The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative**| [CVPR Workshop 2024](https://arxiv.org/abs/2402.14859)| [Github](https://github.com/ChengshuaiZhao0/The-Wolf-Within) ![Star](https://img.shields.io/github/stars/ChengshuaiZhao0/The-Wolf-Within.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.02-red)**Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images** |[COLM 2024](https://arxiv.org/abs/2402.14899)| [Github](https://github.com/aiPenguin/StopReasoning) ![Star](https://img.shields.io/github/stars/aiPenguin/StopReasoning.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.02-red)**Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks**|[arXiv 2024](https://arxiv.org/abs/2402.00626)|[Github](https://github.com/mqraitem/Self-Gen-Typo-Attack)![Star](https://img.shields.io/github/stars/mqraitem/Self-Gen-Typo-Attack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.02-red)**On the Vulnerability of LLM/VLM-Controlled Robotics** | [arXiv 2024](https://arxiv.org/abs/2402.10340)
* ![](https://img.shields.io/badge/abs-2024.01-red)**Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images** | [ICLR 2024](https://arxiv.org/abs/2401.11170)| [Github](https://github.com/KuofengGao/Verbose_Images) ![Star](https://img.shields.io/github/stars/KuofengGao/Verbose_Images.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2024.01-red)**Transferable Multimodal Attack on Vision-Language Pre-training Models** |[S&P 2024](https://www.computer.org/csdl/proceedings-article/sp/2024/313000a102/1Ub239H4xyg)
* ![](https://img.shields.io/badge/abs-2023.12-red)**InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models** | [arXiv 2023](https://arxiv.org/abs/2312.01886)| [Github](https://github.com/xunguangwang/InstructTA) ![Star](https://img.shields.io/github/stars/xunguangwang/InstructTA.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.12-red)**Mutual-modality Adversarial Attack with Semantic Perturbation**| [AAAI 2024](https://arxiv.org/abs/2312.12768)
* ![](https://img.shields.io/badge/abs-2023.12-red)**OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization**| [arXiv 2023](https://arxiv.org/abs/2312.04403)
* ![](https://img.shields.io/badge/abs-2023.12-red)**SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation**|[arXiv 2023](https://arxiv.org/abs/2312.04913)
* ![](https://img.shields.io/badge/abs-2023.12-red)**On the Robustness of Large Multimodal Models Against Image Adversarial Attacks** |[CVPR 2024](https://arxiv.org/abs/2312.03777)|[Github](https://github.com/ChaduCheng/TypoDeceptions) ![Star](https://img.shields.io/github/stars/KuofengGao/Verbose_Images.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.11-red)**How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs**| [ECCV 2024](https://arxiv.org/abs/2311.16101)| [Github](https://github.com/UCSC-VLAA/vllm-safety-benchmark) ![Star](https://img.shields.io/github/stars/UCSC-VLAA/vllm-safety-benchmark.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.11-red)**FigStep: Jailbreaking Large Vision-Language Models via Typographic Visual Prompts**|[AAAI 2025](https://arxiv.org/abs/2311.05608)|[Github](https://github.com/CryptoAILab/FigStep)![Star](https://img.shields.io/github/stars/CryptoAILab/FigStep.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.10-red)**Misusing Tools in Large Language Models With Visual Adversarial Examples** | [arXiv 2023](https://arxiv.org/pdf/2310.03185)
* ![](https://img.shields.io/badge/abs-2023.10-red)**VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models** | [NeurIPS 2023](https://arxiv.org/abs/2310.04655)| [Github](https://github.com/ericyinyzy/VLAttack) ![Star](https://img.shields.io/github/stars/ericyinyzy/VLAttack.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.09-red)**Image Hijacks: Adversarial Images can Control Generative Models at Runtime** | [arXiv 2023](https://arxiv.org/abs/2309.00236)| [Page](https://image-hijacks.github.io/)| [Github](https://github.com/euanong/image-hijacks) ![Star](https://img.shields.io/github/stars/euanong/image-hijacks.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.09-red)**How Robust is Google's Bard to Adversarial Image Attacks?** | [arXiv 2023](https://arxiv.org/abs/2309.11751)| [Github](https://github.com/thu-ml/Attack-Bard) ![Star](https://img.shields.io/github/stars/thu-ml/Attack-Bard.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.08-red)**On the Adversarial Robustness of Multi-Modal Foundation Models** |[ICCV Workshop 2023](https://arxiv.org/abs/2308.10741)
* ![](https://img.shields.io/badge/abs-2023.08-red)**AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning**|[ACM MM 2023](https://arxiv.org/abs/2308.07026)|[Github](https://github.com/CGCL-codes/AdvCLIP)![Star](https://img.shields.io/github/stars/CGCL-codes/AdvCLIP.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.08-red)**Exploring Transferability of Multimodal Adversarial Samples for Vision-Language Pre-training Models with Contrastive Learning**|[TMM 2025](https://arxiv.org/pdf/2308.12636)
* ![](https://img.shields.io/badge/abs-2023.08-red)**Adversarial Illusions in Multi-Modal Embeddings** |[USENIX Security 2024](https://arxiv.org/abs/2308.11804)| [Github](https://github.com/ebagdasa/adversarial_illusions) ![Star](https://img.shields.io/github/stars/ebagdasa/adversarial_illusions.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.07-red)**Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs**|[arXiv 2023](https://arxiv.org/abs/2307.10490)|[Github](https://github.com/ebagdasa/multimodal_injection)![Star](https://img.shields.io/github/stars/ebagdasa/multimodal_injection.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.07-red)**Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models**|[ICCV 2023](https://arxiv.org/abs/2307.14061)
* ![](https://img.shields.io/badge/abs-2023.07-red)**Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models**|[ICLR 2024](https://arxiv.org/abs/2307.14539)
* ![](https://img.shields.io/badge/abs-2023.07-red)**Downstream-agnostic Adversarial Examples**|[ICCV 2023](https://arxiv.org/abs/2307.12280)|[Github](https://github.com/CGCL-codes/AdvEncoder)![Star](https://img.shields.io/github/stars/CGCL-codes/AdvEncoder.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.06-red)**Visual Adversarial Examples Jailbreak Aligned Large Language Models** |[AAAI 2024](https://arxiv.org/abs/2306.13213)| [Github](https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models)![Star](https://img.shields.io/github/stars/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.05-red)**On Evaluating Adversarial Robustness of Large Vision-Language Models** |[NeurIPS 2023](https://arxiv.org/abs/2305.16934)|[Github](https://github.com/yunqing-me/AttackVLM)![Star](https://img.shields.io/github/stars/yunqing-me/AttackVLM.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.04-red)**Defense-Prefix for Preventing Typographic Attacks on CLIP**|[ICCV Workshop 2023](https://arxiv.org/abs/2304.04512)|[Github](https://github.com/azuma164/Defense-Prefix)![Star](https://img.shields.io/github/stars/azuma164/Defense-Prefix.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2023.03-red)**Rethinking Model Ensemble in Transfer-based Adversarial Attacks**|[ICLR 2024](https://arxiv.org/abs/2303.09105)|[Github](https://github.com/huanranchen/AdversarialAttacks)![Star](https://img.shields.io/github/stars/huanranchen/AdversarialAttacks.svg?style=social&label=Star)
* ![](https://img.shields.io/badge/abs-2022.06-red)**Towards Adversarial Attack on Vision-Language Pre-training Models**|[ACM MM 2022](https://arxiv.org/abs/2206.09391)|[Github](https://github.com/adversarial-for-goodness/Co-Attack)![Star](https://img.shields.io/github/stars/adversarial-for-goodness/Co-Attack.svg?style=social&label=Star)
